{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ec6120d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24032e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the project root to the Python path\n",
    "project_root = Path().resolve().parent\n",
    "sys.path.insert(0, str(project_root))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ce69a9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83048b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from config import output_path_prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4b23d285",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval = pd.read_csv(f\"{output_path_prefix}_eval.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6b00541a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d5372c7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>query</th>\n",
       "      <th>answer</th>\n",
       "      <th>page_number</th>\n",
       "      <th>outputs.answer</th>\n",
       "      <th>outputs.page_number</th>\n",
       "      <th>retrieved_contexts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>전 세계에서 초중고 CS 교육을 제공하거나 제공할 계획인 국가의 비율은 얼마이며, ...</td>\n",
       "      <td>전 세계 국가 중 약 3분의 2가 초중고 CS 교육을 제공하거나 제공할 계획이며, ...</td>\n",
       "      <td>21</td>\n",
       "      <td>요약(작성자)와 원본(작성자)\\n- 요약 보고서 작성자: SPRi (SPRi 이슈리...</td>\n",
       "      <td>[21, 22]</td>\n",
       "      <td>### Context #1&lt;document&gt;&lt;page_content&gt;SPRi 이슈리...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0.1  Unnamed: 0  \\\n",
       "0             0           0   \n",
       "\n",
       "                                               query  \\\n",
       "0  전 세계에서 초중고 CS 교육을 제공하거나 제공할 계획인 국가의 비율은 얼마이며, ...   \n",
       "\n",
       "                                              answer  page_number  \\\n",
       "0  전 세계 국가 중 약 3분의 2가 초중고 CS 교육을 제공하거나 제공할 계획이며, ...           21   \n",
       "\n",
       "                                      outputs.answer outputs.page_number  \\\n",
       "0  요약(작성자)와 원본(작성자)\\n- 요약 보고서 작성자: SPRi (SPRi 이슈리...            [21, 22]   \n",
       "\n",
       "                                  retrieved_contexts  \n",
       "0  ### Context #1<document><page_content>SPRi 이슈리...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_eval.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9042b4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = df_eval['query'].tolist()\n",
    "responses = df_eval['outputs.answer'].tolist()\n",
    "references = df_eval['answer'].tolist()\n",
    "relevant_docs = df_eval['retrieved_contexts'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c1ea9f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = []\n",
    "for query, response, reference, relevant_doc in zip(queries, responses, references, relevant_docs):\n",
    "    dataset.append({\n",
    "        \"user_input\": query,\n",
    "        \"response\": response,\n",
    "        \"reference\": reference,\n",
    "        \"retrieved_contexts\": [c.strip() for c in relevant_doc.split(\"### Context\") if c.strip()]\n",
    "    })\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "31d3434d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#1<document><page_content>SPRi 이슈리포트 IS-200\\n\\n\\n  \\nAI Index 2025의 주요 내용 및 시사점\\n\\n7. 교육\\n=====\\n\\n⊙ 세계적으로 AI 및 전산학(Computer Science, CS)8) 교육이 빠르게 확산되고  \\n있으며, AI 전문가 배출도 가속화\\n\\n  \\n\\n⇨ 전 세계 국가 중 3분의 2가 초중고 CS 교육을 제공하거나 제공할 계획  \\n⇨ 미국의 경우, 고등학교 CS 과정에 대한 접근성과 등록률은 이전 학년도보다 약간 증가  \\n했지만 여전히 격차가 존재\\n\\n  \\n\\n※ 연구진은 교육용 AI, AI 역량, AI 교육 등 국가·인종별 전산학 및 AI 교육과정 이용 현황 조사하고,  \\nK-12 과정의 교육 프레임워크 도입과 고등교육과정에서의 인재 양성 지수를 분석\\n\\n□ (AI 교육 도입 확산) ECEP9)에서 인용된 미국 고등학교의 CS 과정을  \\n포함하는 비율은 2017년에는 35%에서 `23년 60%로 증가하여 기초 CS  \\n과정의 중요성을 강조하고 있음\\n\\n![](/images/SPRI_2025_cropped_figure_248.png)\\n\\n※ 자료: Standford HAI, AI Index Report 2025\\n\\n  \\n\\n[그림 7-1] 국가별 CS 교육 제공 여부 맵\\n\\n  \\n\\n◦ (CS 교육 도입) `24년에는 전 세계 국가 중 약 2/3에 해당하는 국가가 CS  \\n과정을 도입하거나 도입할 예정\\n\\n  \\n\\n\\xad 대한민국은 가나, 네덜란드 등과 함께 AI 교육 내용을 공식 커리큘럼에  \\n명시한 소수 국가 중 하나로 인식\\n\\n8) AI는 전통적으로 전산학(CS)의 하위 분야로 연구되어 왔기 때문에, 본 장에서는 AI 관련 자료가 없는 경우 CS 교육  \\n데이터를 참고하여 서술  \\n9) ECEP(Expanding Computing Education Pathways)는 2012년도에 미국 조지아 공과대학의 지원 아래 설립된 단체로,  \\n주(State) 및 지역 교육청의 전산학 교육 확산 정책을 수립을 지원하고 촉진\\n\\n16</page_content><text_summary>nothing</text_summary><image_summary>다국적 지도 형태의 이미지로, 전 세계 각국의 컴퓨터과학(CS) 교육 도입 현황을 색깔로 구분해 보여주는 지도입니다. 아래와 같은 범주별 색상 표기가 포함되어 있습니다.\\n\\n- 진한 파란색: 1차/초등부터 중등까지 CS가 의무화되어 있음\\n- 연한 파란색: 1차 또는 2차 중 한 곳에서 CS가 의무화되어 있음\\n- 청록/하늘색 계열: CS가 전적으로(전 과정에서) 선택 과목으로 제공됨\\n- 보라색: 일부 학교나 학군에서 CS를 도입\\n- 분홍색: CS를 교과 간 학제적으로 다루는 교과과정(크로스 커리큘럼)\\n- 회색: CS 도입이 계획되었음을 나타냄\\n- 흰색/없음 색상: CS가 전혀 도입되지 않음\\n\\n캡션과 맥락\\n- 그림 제목: [그림 7-1] 국가별 CS 교육 제공 여부 맵\\n- 자료 출처: Standford HAI, AI Index Report 2025\\n- 화면의 주된 메시지: 전 세계적으로 CS 교육의 도입이 확산 중이며, 각국의 정책 형태가 다양하게 나타남. 또한 대한민국은 AI 교육 내용을 공식 커리큘럼에 명시한 소수 국가 중 하나라는 주석이 함께 제시되어 있습니다.</image_summary></document>',\n",
       " '#2<document><page_content>SPRi 이슈리포트 IS-200\\n\\n\\n  \\nAI Index 2025의 주요 내용 및 시사점\\n\\n□ (AI 교육 표준화) 표준화된 AI 교육 프레임워크 도입을 통한 AI 구현 방법에  \\n대한 지침을 제공 중\\n\\n◦ (지침 발표 동향) `24년 11월 10개국\\\\*에서 교육용 AI에 대한 지침을 발표\\n================================================\\n\\n  \\n\\n\\\\* 뉴질랜드, 벨기에, 우루과이, 우크라이나, 영국, 일본, 캐나다, 한국, 호주, 미국\\n\\n  \\n\\n\\xad 미국의 경우, K-12 CS 표준을 채택한 44개 주(State)는 CSTA10) 표준과  \\n평균적으로 97%의 개념 일치율을 보이며, 그중에서 33개 주는 AI 관련  \\n표준도 함께 보유\\n\\n□ (인재 쏠림 현상) 미국 내 전산학 및 AI 관련 유학생 수의 꾸준한 증가 및  \\n학위 졸업자의 폭발적인 증가\\n\\n◦ (국적) 미국 내 전산학 관련 고등교육과정에 등록한 학생들의 과반이  \\n비(非)미국 국적자이며, 매년 꾸준히 증가\\n\\n◦ (CS 학위) IPEDS11)의 통계에 따르면 `22~`23년 동안 미국 내 전산학 관련  \\n유학생의 석사 학위 수여자 수는 약 두 배 증가(15,811명 → 34,580명)\\n\\n  \\n\\n\\\\* 미국 내 유학생 중 인도와 중국 국적의 학생이 석사과정(92,130명) 중 93%,  \\n박사과정(13,070명) 중 60%를 차지\\n\\n◦ (졸업자 수) `22년 미국 내 신규 ICT 학위 졸업생은 2위 국가보다 두 배\\\\*  \\n이상 많은 학·석·박사 과정 졸업생을 배출함\\n\\n  \\n\\n\\\\* `22년 OECD 통계 기준,  \\n△(학사) 1위 미국(116,401명), 2위 브라질(61,760명), … 6위 한국(19,603명) 순,  \\n△(석사) 1위 미국(55,706명), 2위 영국(21,688명), … 12위 한국 (2,910명) 순,  \\n△(박사) 1위 미국(2,759명), 2위 영국 (1,156명), … 5위 한국 (617명) 순</page_content><text_summary>nothing</text_summary><image_summary>nothing</image_summary></document>']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]['retrieved_contexts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7fecfb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas import EvaluationDataset\n",
    "evaluation_dataset = EvaluationDataset.from_list(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2e84a5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI(model=\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f609caaf",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "All metrics must be initialised metric objects, e.g: metrics=[BleuScore(), AspectCritic()]",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[44]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m llm = llm_factory(\u001b[33m'\u001b[39m\u001b[33mgpt-5-mini\u001b[39m\u001b[33m'\u001b[39m, client=OpenAI())\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mragas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcollections\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ContextRecall, Faithfulness, FactualCorrectness\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m result = \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mevaluation_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mContextRecall\u001b[49m\u001b[43m(\u001b[49m\u001b[43mllm\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mFaithfulness\u001b[49m\u001b[43m(\u001b[49m\u001b[43mllm\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mFactualCorrectness\u001b[49m\u001b[43m(\u001b[49m\u001b[43mllm\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mllm\u001b[49m\u001b[43m=\u001b[49m\u001b[43mllm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/RAG-end2end/.venv/lib/python3.13/site-packages/ragas/_analytics.py:278\u001b[39m, in \u001b[36mtrack_was_completed.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    275\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[32m    276\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args: P.args, **kwargs: P.kwargs) -> T:\n\u001b[32m    277\u001b[39m     track(IsCompleteEvent(event_type=func.\u001b[34m__name__\u001b[39m, is_completed=\u001b[38;5;28;01mFalse\u001b[39;00m))\n\u001b[32m--> \u001b[39m\u001b[32m278\u001b[39m     result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    279\u001b[39m     track(IsCompleteEvent(event_type=func.\u001b[34m__name__\u001b[39m, is_completed=\u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[32m    281\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/RAG-end2end/.venv/lib/python3.13/site-packages/ragas/evaluation.py:484\u001b[39m, in \u001b[36mevaluate\u001b[39m\u001b[34m(dataset, metrics, llm, embeddings, experiment_name, callbacks, run_config, token_usage_parser, raise_exceptions, column_map, show_progress, batch_size, _run_id, _pbar, return_executor, allow_nest_asyncio)\u001b[39m\n\u001b[32m    480\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    481\u001b[39m     \u001b[38;5;66;03m# Default behavior: use nest_asyncio for backward compatibility (Jupyter notebooks)\u001b[39;00m\n\u001b[32m    482\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mragas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01masync_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m run\n\u001b[32m--> \u001b[39m\u001b[32m484\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_async_wrapper\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/RAG-end2end/.venv/lib/python3.13/site-packages/ragas/async_utils.py:156\u001b[39m, in \u001b[36mrun\u001b[39m\u001b[34m(async_func, allow_nest_asyncio)\u001b[39m\n\u001b[32m    148\u001b[39m     loop_type = \u001b[38;5;28mtype\u001b[39m(loop).\u001b[34m__name__\u001b[39m\n\u001b[32m    149\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    150\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCannot execute nested async code with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloop_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    151\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33muvloop does not support nested event loop execution. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    152\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPlease use asyncio\u001b[39m\u001b[33m'\u001b[39m\u001b[33ms standard event loop in Jupyter environments, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    153\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mor refactor your code to avoid nested async calls.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    154\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m156\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43masyncio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcoro\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/RAG-end2end/.venv/lib/python3.13/site-packages/nest_asyncio.py:30\u001b[39m, in \u001b[36m_patch_asyncio.<locals>.run\u001b[39m\u001b[34m(main, debug)\u001b[39m\n\u001b[32m     28\u001b[39m task = asyncio.ensure_future(main)\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     32\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m task.done():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/RAG-end2end/.venv/lib/python3.13/site-packages/nest_asyncio.py:98\u001b[39m, in \u001b[36m_patch_loop.<locals>.run_until_complete\u001b[39m\u001b[34m(self, future)\u001b[39m\n\u001b[32m     95\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f.done():\n\u001b[32m     96\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m     97\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mEvent loop stopped before Future completed.\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m98\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.13.8-linux-x86_64-gnu/lib/python3.13/asyncio/futures.py:199\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    197\u001b[39m \u001b[38;5;28mself\u001b[39m.__log_traceback = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    198\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m199\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception.with_traceback(\u001b[38;5;28mself\u001b[39m._exception_tb)\n\u001b[32m    200\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.13.8-linux-x86_64-gnu/lib/python3.13/asyncio/tasks.py:304\u001b[39m, in \u001b[36mTask.__step_run_and_handle_result\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m    300\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    301\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    302\u001b[39m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[32m    303\u001b[39m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m304\u001b[39m         result = \u001b[43mcoro\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    305\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    306\u001b[39m         result = coro.throw(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/RAG-end2end/.venv/lib/python3.13/site-packages/ragas/evaluation.py:457\u001b[39m, in \u001b[36mevaluate.<locals>._async_wrapper\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    456\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_async_wrapper\u001b[39m():\n\u001b[32m--> \u001b[39m\u001b[32m457\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m aevaluate(\n\u001b[32m    458\u001b[39m         dataset=dataset,\n\u001b[32m    459\u001b[39m         metrics=metrics,\n\u001b[32m    460\u001b[39m         llm=llm,\n\u001b[32m    461\u001b[39m         embeddings=embeddings,\n\u001b[32m    462\u001b[39m         experiment_name=experiment_name,\n\u001b[32m    463\u001b[39m         callbacks=callbacks,\n\u001b[32m    464\u001b[39m         run_config=run_config,\n\u001b[32m    465\u001b[39m         token_usage_parser=token_usage_parser,\n\u001b[32m    466\u001b[39m         raise_exceptions=raise_exceptions,\n\u001b[32m    467\u001b[39m         column_map=column_map,\n\u001b[32m    468\u001b[39m         show_progress=show_progress,\n\u001b[32m    469\u001b[39m         batch_size=batch_size,\n\u001b[32m    470\u001b[39m         _run_id=_run_id,\n\u001b[32m    471\u001b[39m         _pbar=_pbar,\n\u001b[32m    472\u001b[39m         return_executor=return_executor,\n\u001b[32m    473\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/RAG-end2end/.venv/lib/python3.13/site-packages/ragas/evaluation.py:133\u001b[39m, in \u001b[36maevaluate\u001b[39m\u001b[34m(dataset, metrics, llm, embeddings, experiment_name, callbacks, run_config, token_usage_parser, raise_exceptions, column_map, show_progress, batch_size, _run_id, _pbar, return_executor)\u001b[39m\n\u001b[32m    128\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m    129\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mMetrics should be provided in a list, e.g: metrics=[BleuScore()]\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    130\u001b[39m     )\n\u001b[32m    132\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(metrics, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(m, Metric) \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m metrics):\n\u001b[32m--> \u001b[39m\u001b[32m133\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m    134\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAll metrics must be initialised metric objects, e.g: metrics=[BleuScore(), AspectCritic()]\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    135\u001b[39m     )\n\u001b[32m    137\u001b[39m \u001b[38;5;66;03m# default metrics\u001b[39;00m\n\u001b[32m    138\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m metrics \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mTypeError\u001b[39m: All metrics must be initialised metric objects, e.g: metrics=[BleuScore(), AspectCritic()]"
     ]
    }
   ],
   "source": [
    "from ragas import evaluate\n",
    "from ragas.llms import llm_factory\n",
    "from openai import OpenAI\n",
    "\n",
    "llm = llm_factory('gpt-5-mini', client=OpenAI())\n",
    "from ragas.metrics.collections import ContextRecall, Faithfulness, FactualCorrectness\n",
    "\n",
    "result = evaluate(dataset=evaluation_dataset,metrics=[ContextRecall(llm), Faithfulness(llm), FactualCorrectness(llm)], llm=llm)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10837843",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
